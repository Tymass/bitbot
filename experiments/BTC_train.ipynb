{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLATIONS DATA ACQUISITION AND IMPORTS"
      ],
      "metadata": {
        "id": "DXVm-ulVjCb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VuGzVKrGeE_"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "! kaggle datasets download -qd swaptr/bitcoin-historical-data\n",
        "! mkdir btc && unzip -q bitcoin-historical-data.zip -d btc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ds8CACQGlTx"
      },
      "outputs": [],
      "source": [
        "! pip install -q neuralforecast statsforecast datasetsforecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAOxc5i0GqBP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from utilsforecast.losses import smape, rmse, mae, mse\n",
        "from utilsforecast.evaluation import evaluate\n",
        "from statsforecast import StatsForecast\n",
        "\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.auto import TFT, LSTM, NBEATS, BiTCN, TimesNet, DeepAR\n",
        "from neuralforecast.losses.pytorch import MAE\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from ray import tune\n",
        "import optuna\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrhVzijhSa3l",
        "outputId": "c54cbd86-b008-4fa8-a498-9b777fde631c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# we store data on gdrive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HN7v69OMHImy"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('btc/data.csv')\n",
        "\n",
        "invalid_date = '2017-01-01 00:00:00'\n",
        "index = data.index[data['Date'] == invalid_date].tolist()\n",
        "\n",
        "df = data.truncate(after=index[0])\n",
        "\n",
        "df = df[['Date', 'Close']]\n",
        "\n",
        "df = df.sort_values(\"Date\", ascending=True)\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_size = 0.90\n",
        "split_index = int(df.shape[0] * train_size)\n",
        "\n",
        "df['Close'] = scaler.fit_transform(df[['Close']])\n",
        "\n",
        "aa = len(df) - split_index\n",
        "test = df.tail(aa)\n",
        "train = df.head(split_index)\n",
        "\n",
        "horizon = 5\n",
        "\n",
        "train.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)\n",
        "test.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)\n",
        "\n",
        "train['unique_id'] = 1\n",
        "test['unique_id'] = 1\n",
        "\n",
        "\n",
        "\n",
        "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
        "\n",
        "StatsForecast.plot(test, engine='matplotlib')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELS INITIALIZATION"
      ],
      "metadata": {
        "id": "wLNUWWofjOoq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhtTswCjHM58"
      },
      "outputs": [],
      "source": [
        "# done\n",
        "lstm = LSTM(h=horizon, input_size=25, inference_input_size=-5,\n",
        "       encoder_n_layers=1, encoder_hidden_size=50,\n",
        "       context_size=5, decoder_hidden_size=64,\n",
        "       loss=MAE(), valid_loss=None,\n",
        "       max_steps=2000, learning_rate=0.00019058032335399208,\n",
        "       batch_size=16,\n",
        "       scaler_type='robust', random_seed=1\n",
        ")\n",
        "\n",
        "# done\n",
        "bitcn = BiTCN(h=horizon, input_size=25, hidden_size=32, dropout= 0.12848839287344987,\n",
        "        loss=MAE(), max_steps=2000, learning_rate=0.005652625486826798,\n",
        "        batch_size=256,\n",
        "        windows_batch_size=512, inference_windows_batch_size=512,\n",
        "        step_size=5, scaler_type='standard', random_seed=18\n",
        ")\n",
        "\n",
        "#done\n",
        "nbeats = NBEATS(h=horizon, input_size=15, loss=MAE(), valid_loss=None,\n",
        "         max_steps=1000, learning_rate=0.007920715236497127,\n",
        "         batch_size=32, windows_batch_size=256,\n",
        "         step_size=5, scaler_type='standard', random_seed=1,\n",
        ")\n",
        "\n",
        "#done\n",
        "tft = TFT(h=horizon, input_size=15, hidden_size=64, loss=MAE(),\n",
        "      max_steps=1000, learning_rate=0.00538096845409797,\n",
        "      batch_size=32,\n",
        "      windows_batch_size=256, start_padding_enabled=False,\n",
        "      step_size=5, scaler_type='robust', random_seed=12\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGH2DFBQEElJ"
      },
      "outputs": [],
      "source": [
        "# training without hyperparameters optimization is less costly so we can train all models at once\n",
        "\n",
        "models = [\n",
        "    bitcn, nbeats, tft, lstm\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpxvkkP0Div_"
      },
      "outputs": [],
      "source": [
        "# build project files structure:\n",
        "#    -content\n",
        "#      -models\n",
        "#        -data\n",
        "#        -plots\n",
        "\n",
        "pwd = '/content/models'\n",
        "models_plots = os.path.join(pwd, 'plots')\n",
        "models_data = os.path.join(pwd, 'data')\n",
        "\n",
        "os.mkdir(pwd)\n",
        "os.mkdir(models_plots)\n",
        "os.mkdir(models_data)\n",
        "\n",
        "for model in models:\n",
        "    path = os.path.join(models_plots, str(model))\n",
        "    os.mkdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbmv-QWLfNth"
      },
      "outputs": [],
      "source": [
        "# training with cross validation\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "nf = NeuralForecast(models=models, freq='T')\n",
        "\n",
        "cv_df = nf.cross_validation(train, n_windows=10)\n",
        "\n",
        "# or\n",
        "# nf.fit(train, val_size=2*horizon)\n",
        "\n",
        "nf.save(models_data, overwrite=True, save_dataset=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VZIyxn8VDvN"
      },
      "outputs": [],
      "source": [
        "# save trained models to gdrive\n",
        "\n",
        "!cp /content/models/data/* /content/drive/MyDrive/models/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2CJoEp0-IgX"
      },
      "outputs": [],
      "source": [
        "# perform post training evaluation\n",
        "\n",
        "df_path = f'{pwd}/evaluation.csv'\n",
        "\n",
        "evaluation_df = evaluate(cv_df.loc[:, cv_df.columns != 'cutoff'], metrics=[rmse, mae, mse])\n",
        "evaluation_df['best_model'] = evaluation_df.drop(columns=['metric', 'unique_id']).idxmin(axis=1)\n",
        "print(evaluation_df)\n",
        "\n",
        "evaluation_df.to_csv(df_path)\n",
        "\n",
        "summary_df = evaluation_df.groupby(['metric', 'best_model']).size().sort_values().to_frame()\n",
        "summary_df = summary_df.reset_index()\n",
        "summary_df.columns = ['metric', 'model', 'num. of unique_ids']\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAR_GSSGVmoa"
      },
      "outputs": [],
      "source": [
        "# save trained models to gdrive\n",
        "\n",
        "!cp /content/models/plots/* /content/drive/MyDrive/models/plots/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}